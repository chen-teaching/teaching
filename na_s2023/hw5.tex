\documentclass[12pt]{article}

\usepackage[margin=1.25in]{geometry}

\usepackage{graphicx}
\usepackage{amsmath,amsthm,amssymb}

% you will probably have to remove the font-setup
\usepackage[no-math]{fontspec}
\setmainfont[
    BoldFont = Vollkorn Bold,
    ItalicFont = Vollkorn Italic,
    BoldItalicFont={Vollkorn Bold Italic},
    RawFeature=+lnum,
]{Vollkorn}

\setsansfont[
    BoldFont = Lato Bold,
    FontFace={l}{n}{*-Light},
    FontFace={l}{it}{*-Light Italic},
]{Lato}

\usepackage{unicode-math}
\mathitalicsmode=1

\setmathfont[
mathit = sym,
mathup = sym,
mathbf = sym,
math-style = TeX, 
bold-style = TeX
]{Wholegrain Math}
\everydisplay{\Umathoperatorsize\displaystyle=4ex}
\AtBeginDocument{\renewcommand\setminus{\smallsetminus}}
% font setup ends here

\newcommand\extrafootertext[1]{%
    \bgroup
    \renewcommand\thefootnote{\fnsymbol{footnote}}%
    \renewcommand\thempfootnote{\fnsymbol{mpfootnote}}%
    \footnotetext[0]{#1}%
    \egroup
}

\theoremstyle{definition}
\newtheorem{problem}{Problem}
\newenvironment{solution}
  {\textbf{Solution.} }
  {}

\usepackage{listings}
\lstset{basicstyle=\ttfamily,breaklines=true}

\usepackage{enumitem}
\setlist[itemize]{itemsep=-.15em, topsep=-.5em}
\setlist[enumerate]{itemsep=-.15em, topsep=-.5em}
\setlist[enumerate,1]{label={(\alph*)}}

\PassOptionsToPackage{hyphens}{url}
\usepackage{hyperref}

\setlength{\parskip}{1em}
\setlength{\parindent}{0em}

\renewcommand{\d}{\mathrm{d}}
\renewcommand{\vec}{\mathbf}
\newcommand{\T}{\mathsf{T}}

\usepackage{dsfont}
\newcommand{\bOne}{\mathds{1}}

\newcommand{\F}{\mathsf{F}}

\usepackage{array,booktabs}

\begin{document}

    \textbf{\Large\sffamily{Homework 4\hfill Numerical Analysis Spring 2023}}
    
    \vspace{-1.8em}
    \hrulefill
 
\textbf{Instructions:}
    \begin{itemize}
        \item Due 04/06 at 11:59pm on \href{https://www.gradescope.com/courses/487363/}{Gradescope}.
        \item Write the names of anyone you work with on the top of your assignment. If you worked alone, write that you worked alone.
        \item Show your work.
        \item Include all code you use as copyable \verb|monospaced| text in the PDF (i.e. not as a screenshot).
        \item Do not put the solutions to multiple problems on the same page.
        \item Tag your responses on gradescope. Each page should have a \emph{single} problem tag. Improperly tagged responses will not receive credit.
\end{itemize}
    
\vspace{2em}

\begin{problem}

    Download the numpy data file from this link:  \url{https://drive.google.com/file/d/1Ao0HQDnaGlYirr8pak6TRUd8IItqTy9U/view?usp=share_link}

Use the following code to import the file into numpy.

\begin{lstlisting}
import numpy as np
import matplotlib.pyplot as plt

im = np.load('change this path/tandon.npy')
A = np.mean(im,axis=2)
\end{lstlisting}

Here we obtain $\vec{A}$ by averaging the red, green, and blue channels of the image.
This result sin a black and white image.

    \begin{enumerate}
        \item Plot the image using \lstinline{plt.imshow}.
            You may want to use the colormap \texttt{'Greys\_r'} so that it looks like a greyscale image.
        \item
            Compute the reduced SVD of $\vec{A}$. 
            You can use \lstinline{full_matrices=False} to get the reduced SVD.
            This will be much faster than computing the full SVD.

            For each $k=1,10,100,200$, make a plot of the best rank-$k$ approximation $\vec{A}_k$ to $\vec{A}$ (i.e. via truncated SVD).
            Label each plot with the rank $k$ as well as the relative error $\| \vec{A} - \vec{A}_k \|_\F / \| \vec{A} \|_\F$
        \item 
            Remark on the quality of the plots. 
            
            How many floating point numbers are required to store $\vec{A}$?
            How many are required to store the rank-$k$ truncated SVD?
    \end{enumerate}

\end{problem}
\clearpage

\begin{problem}
    Computing the SVD is expensive, but randomization can help us!
    \begin{enumerate}
        \item Randomized numerical linear algebra (RandNLA) is the study of the use of randomness in numerical linear algebra algorithms. 
            One of the most famous randNLA algorithms is the randomized SVD.
            A simple version for approximating the SVD of a $m\times n$ matrix $\vec{A}$ can be described in several lines:
            \begin{itemize}
                \item Choose a $n\times k$ matrix $\vec{R}$ with standard normal random entries
                \item Compute $\vec{X} = \vec{A} \vec{R}$
                \item Compute $\vec{Q},\_ = \textsc{QR}(\vec{X})$
                \item Compute SVD of $\vec{Q}^\T \vec{A}$: $\hat{\vec{U}} \hat{\vec{\Sigma}} \hat{\vec{V}}^\T$
                \item Return approximate SVD of $\vec{A}$: $(\vec{Q} \hat{\vec{U}}) \hat{\vec{\Sigma}} \hat{\vec{V}}^\T$  
            \end{itemize}

            Implement this algorithm and time it for $k=100$ with the same matrix $\vec{A}$ as in problem 1.
            To generate the random matrix, you can use \lstinline{np.random.randn(n,k)}.


            Again make sure to use \lstinline{full_matrices=False} when computing the SVD of $\vec{Q}^\T \vec{A}$.
            Compare this to long the whole randomized SVD took (all of the steps) against the time to compute the exact SVD in the previous problem.
        
        \item 
            Make a plot of the rank $k=100$ truncated SVD (from problem 1) and the $k=100$ randomized SVD.
            Show the relative errors $\|\vec{A} - (\vec{Q} \hat{\vec{U}})\hat{\vec{\Sigma}} \hat{\vec{V}}^\T \|_\F/\|\vec{A}\|_\F$ for each. 

        \item Prove that $\vec{Q}\hat{\vec{U}}$ has orthonormal columns.
    \end{enumerate}
\end{problem}

\begin{problem}
    \begin{enumerate}
        \item Suppose $\vec{v}$ is an eigenvector of $\vec{A}$ with eigenvalue $\lambda$. Show $c\vec{v}$ is an eigenvector of $\vec{A}$. What is the eigenvalue?
        \item Suppose $\vec{X}$ is a $n\times m$ matrix. 
            Write $\|\vec{X}\|_\F$ in terms of the column-norms $\|[\vec{X}]_{:,i}\|_2$.
        \item Suppose $\vec{X}$ is a $n\times m$ matrix and $\vec{U}$ is a $n\times n$ orthogonal matrix $(\vec{U}^\T \vec{U} = \vec{I})$.
            Show that $\|\vec{U} \vec{X}\|_\F = \| \vec{X} \|_\F$. 
            Hint: use (b) and show that $\|\vec{U} \vec{x}\|_2 = \|\vec{x}\|_2$ for any vector $\vec{x}$.


    \end{enumerate}
\end{problem}

\begin{problem}

This problem will illustrate that solving the normal equations is less stable than other approaches.

\begin{enumerate}
    \item 
    For each $\kappa = 10^1,10^2,10^3 \ldots, 10^8$, construct a $500\times 100$ matrix $\vec{A}$ whose condition number is $\kappa$.
        A simple way to do this is to generate $\vec{U}$ and $\vec{V}$ as random orthogonal matrices of size $m\times n$ and $n\times n$ and define $\vec{\Sigma}$ as a $n\times n$ diagonal matrix manually.
    
The following code gets you started, you just need to modify the line for the singular values \lstinline{s = ...}.
\begin{lstlisting}
m,n = 500,100
U,_ = np.linalg.qr(np.random.randn(m,n))
V,_ = np.linalg.qr(np.random.randn(n,n))
s = #TODO
    
A = U@np.diag(s)@V.T
\end{lstlisting}


        Let $\vec{b}$ be the all ones vector, and compute the ``true'' solution to the least squares problem $\min_{\vec{x}} \| \vec{b} - \vec{A} \vec{x}\|_2$  by setting 
    $\vec{x}_{\textup{true}} =  \vec{V} \vec{\Sigma}^{-1} \vec{U}^\T \vec{b}$.
This can be done with the following code:
\begin{lstlisting}
b = np.ones(m)
x_true = V@np.diag(1/s)@U.T@b
\end{lstlisting}


Now, compute the least squares solution via:
\begin{itemize}
    \item numpy's least squares solver \lstinline{np.linalg.lstsq}
    \item a QR based approach with numpy's \lstinline{np.linalg.qr} and \lstinline{np.linalg.solve} or \lstinline{sp.linalg.solve_triangular}
    \item Solving the normal equations with \lstinline{np.linalg.solve}
\end{itemize}

    For each of these three methods and each value of $\kappa$, record the relative error $\| \vec{x}_{\textup{method}} - \vec{x}_{\textup{true}} \|_2 / \| \vec{x}_{\textup{true}} \|_2$, where $\vec{x}_{\textup{method}}$ is the solution obtained by the given method.

\item 
    Make a log-log plot with the following five (labeled) curves:
    \begin{itemize}
        \item $\kappa$ vs $10^{-16} \kappa$
        \item $\kappa$ vs $10^{-16} \kappa^2$
        \item $\kappa$ vs relative error (for each of the three methods above)
    \end{itemize}

    Comment on what you observe about the plots. 
    In particular, discuss how each method depends on $\kappa$ and what the relative errors would be if we did everything in exact arithmetic

\end{enumerate}
\end{problem}

\end{document}
